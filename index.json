
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a PhD candidate at the RLAI Lab at the University of Alberta, working with Professor Martha White. I am interested in sequential decision making for real world problems. Specifically, my research focuses on offline reinforcement learning and off-policy learning.\n","date":1672790400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687639588,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD candidate at the RLAI Lab at the University of Alberta, working with Professor Martha White. I am interested in sequential decision making for real world problems. Specifically, my research focuses on offline reinforcement learning and off-policy learning.","tags":null,"title":"Vincent Liu","type":"authors"},{"authors":[],"categories":null,"content":"","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"d487e7727c354b66f39a6892e10b87d8","permalink":"https://vincentliu3.github.io/talks/offline_policy_selection/_index/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/offline_policy_selection/_index/","section":"talks","summary":"","tags":[],"title":"When is Offline Policy Selection Feasible for Reinforcement Learning?","type":"talks"},{"authors":[],"categories":[],"content":"This post is about gradient estimation for policy gradient methods.\nConsider the problem of estimating the gradient of an expectation $\\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{x\\sim p_\\theta}[f(x)]$ where $f$ is a function, $x$ is a random variable and $p$ is a parameterized distribution.\nThere are two main ways to estimate the gradient (see a summary in [1]). The first one is the likelihood ratio estimator: $$ \\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{x\\sim p_\\theta}\\left[f(x)\\right] = \\mathrm{E}_{x\\sim p_\\theta}\\left[f(x)\\frac{\\partial}{\\partial\\theta}\\log p_\\theta(x)\\right]. $$ The second one is the pathwise derivative estimator, or the reparameterization trick. Let $x=g_\\theta(z)$ be a deterministic and differentiable function of $\\theta$ and another random variable $z$ that we can sample:\n$$ \\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{z}\\left[f(g_\\theta(z))\\right] = \\mathrm{E}_{z}\\left[\\frac{\\partial}{\\partial\\theta}f(g_\\theta(z))\\right]. $$ Now let’s move back to reinforcement learning.\nREINFORCE [2] uses the likelihood ratio estimator. Fix a state $s$, the policy $\\pi_\\theta(a|s)$ is the parameterized distribution where $a$ is a random variable, $Q$ is any function of the state-action pair. We want to estimate $\\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{a\\sim \\pi_\\theta(\\cdot|s)}[Q(s.a)]$: $$ \\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{a\\sim \\pi_\\theta(\\cdot|s)}\\left[Q(s,a)\\right] = \\mathrm{E}_{a\\sim \\pi_\\theta(\\cdot|s)}\\left[Q(s,a)\\frac{\\partial}{\\partial\\theta}\\log \\pi_\\theta(a|s)\\right]. $$ DPG [3] and SAC [4] use the reparameterization trick. Let $g_\\theta(z,s)$ be the output of the policy where $z$ is a random variable: $$\\begin{align} \\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{z}\\left[Q(s,g_\\theta(z,s))\\right] \u0026amp;= \\mathrm{E}_{z}\\left[\\frac{\\partial}{\\partial\\theta}Q(s,g_\\theta(z,s))\\right]\\\\ \u0026amp;= \\mathrm{E}_{z}\\left[\\frac{\\partial}{\\partial a}Q(s,a) |_{a=g_\\theta(z,s)} \\frac{\\partial}{\\partial \\theta}g_\\theta(z,s)\\right]\\\\ \\end{align}$$ The second equation follows from the chain rule.\nSAC has an additional term, but we can still estimate the gradient. $$\\begin{align} \u0026amp;\\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{z}\\left[\\alpha\\log\\pi_\\theta(g_\\theta(z,s)|s) + Q(s,g_\\theta(z,s))\\right] \\\\ % \u0026amp;=\\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{z}\\left[\\alpha\\log\\pi_\\theta(g_\\theta(z,s)|s)\\right] + \\frac{\\partial}{\\partial\\theta}\\mathrm{E}_{z}\\left[ Q(s,g_\\theta(z,s))\\right] \\\\ \u0026amp;=\\mathrm{E}_{z}\\Big[\\frac{\\partial}{\\partial\\theta}\\alpha\\log\\pi_\\theta(a|s)|_{a=g_\\theta(z,s)} \\\\ \u0026amp;\\ \\ \\ \\ + \\frac{\\partial}{\\partial a} \\alpha\\log\\pi_\\theta(a|s)|_{a=g_\\theta(z,s)}\\frac{\\partial}{\\partial \\theta}g_\\theta(z,s) \\\\ \u0026amp;\\ \\ \\ \\ +\\frac{\\partial}{\\partial a}Q(s,a) |_{a=g_\\theta(z,s)} \\frac{\\partial}{\\partial \\theta}g_\\theta(z,s)\\Big] \\\\ \\end{align}$$ I provide some Pytorch examples with Gaussian policies below. The Pytorch implementation can be found here.\nThe likelihood ratio estimator:\nmean, std = policy_network(state) m = torch.distributions.Normal(mean, std) action = m.sample() log_prob = m.log_prob(action) q_value = critic_network(state, action).detach() loss = -log_prob * q_value loss.backward() The reparameterization trick:\nmean, std = policy_network(state) m = torch.distributions.Normal(mean, std) action = m.rsample() log_prob = m.log_prob(action) q_value = critic_network(state, action) # critic_network needs to be differentiable loss = -q_value loss.backward() [1] Gradient Estimation Using Stochastic Computation Graphs\n[2] Simple statistical gradient-following algorithms for connectionist reinforcement learning\n[3] Deterministic Policy Gradient Algorithms\n[4] Soft Actor-Critic Algorithms and Applications\n","date":1690502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690502400,"objectID":"3debaccca6b5deb915008008ea5252a7","permalink":"https://vincentliu3.github.io/posts/gradient_estimation/","publishdate":"2023-07-28T00:00:00Z","relpermalink":"/posts/gradient_estimation/","section":"posts","summary":"A short note on gradient estimation for policy gradient methods.","tags":[],"title":"Gradient Estimation for Policy Gradient Methods","type":"posts"},{"authors":[],"categories":[],"content":"Awesome courses on RL and ML with online access:\nCMPUT 653 RL theory by Csaba Szepesvári (with lecture notes \u0026amp; videos) Statistical RL by Nan Jiang (with lecture notes \u0026amp; videos) Statistical Machine Learning by Ryan Tibshirani and Larry Wasserman (with lecture notes \u0026amp; videos) Introduction to RL by Martha White and Adam White (Coursera) Awesome courses on statistics with online access:\nSTAT 571 Probability and Measure by Adam B Kashlak (with lecture notes \u0026amp; videos) High-Dimensional Probability by Roman Vershynin (with lecture notes \u0026amp; videos) Uncertain: Modern Topics in Uncertainty Estimation by Aaron Roth (with lecture notes \u0026amp; videos) Awesome books on RL and ML with online access:\nBandit Algorithms by Tor Lattimore and Csaba Szepesvári Reinforcement Learning: Theory and Algorithms by Alekh Agarwal, Nan Jiang, Sham Kakade, and Wen Sun Learning Theory from First Principles by Francis Bach Mathematical Analysis of Machine Learning Algorithms by Tong Zhang Understanding Machine Learning: From Theory to Algorithms ","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690416000,"objectID":"084e7be9e4acfae54af9219773fb7cbb","permalink":"https://vincentliu3.github.io/posts/resources/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/posts/resources/","section":"posts","summary":"A list of courses on RL and ML with online access.","tags":[],"title":"A list of courses and books on RL and ML with online access","type":"posts"},{"authors":["Vincent Liu","Prabhat Nagarajan","Andrew Patterson","Martha White"],"categories":[],"content":"","date":1672790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687639205,"objectID":"199f9d07fe816fe31a3ff8865c2e46e6","permalink":"https://vincentliu3.github.io/publication/liu-2023-when/","publishdate":"2023-06-24T20:40:04.245889Z","relpermalink":"/publication/liu-2023-when/","section":"publication","summary":"","tags":[],"title":"When is Offline Policy Selection Feasible for Reinforcement Learning?","type":"publication"},{"authors":["Vincent Liu","James R Wright","Martha White"],"categories":[],"content":"","date":1672704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593514,"objectID":"28864c4c08e285c3b235bf90cf686cee","permalink":"https://vincentliu3.github.io/publication/liu-2023-exploiting/","publishdate":"2023-06-12T18:11:54.545935Z","relpermalink":"/publication/liu-2023-exploiting/","section":"publication","summary":"","tags":[],"title":"Exploiting action impact regularity and exogenous state variables for offline reinforcement learning","type":"publication"},{"authors":["Vincent Liu","Yash Chandak","Philip Thomas","Martha White"],"categories":[],"content":"","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593515,"objectID":"59b7963b2dc944346be16840cc0b4174","permalink":"https://vincentliu3.github.io/publication/liu-2023-asymptotically/","publishdate":"2023-06-12T18:11:54.897528Z","relpermalink":"/publication/liu-2023-asymptotically/","section":"publication","summary":"","tags":[],"title":"Asymptotically Unbiased Off-Policy Policy Evaluation when Reusing Old Data in Nonstationary Environments","type":"publication"},{"authors":["Vincent Liu","Han Wang","Ruo Yu Tao","Khurram Javed","Adam White","Martha White"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687639588,"objectID":"5d1d7f5f164af0e6b7690d722d7b6f51","permalink":"https://vincentliu3.github.io/publication/liu-2023-measuring/","publishdate":"2023-06-24T20:46:28.002333Z","relpermalink":"/publication/liu-2023-measuring/","section":"publication","summary":"","tags":[],"title":"Measuring and Mitigating Interference in Reinforcement Learning","type":"publication"},{"authors":["Han Wang","Erfan Miahi","Martha White","Marlos C Machado","Zaheer Abbas","Raksha Kumaraswamy","Vincent Liu","Adam White"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593515,"objectID":"518c8df2bfa7e4ed6774e758bb08f2ea","permalink":"https://vincentliu3.github.io/publication/wang-2022-investigating/","publishdate":"2023-06-12T18:11:55.609248Z","relpermalink":"/publication/wang-2022-investigating/","section":"publication","summary":"","tags":[],"title":"Investigating the properties of neural network representations in reinforcement learning","type":"publication"},{"authors":["Han Wang","Archit Sakhadeo","Adam M White","James M Bell","Vincent Liu","Xutong Zhao","Puer Liu","Tadashi Kozuno","Alona Fyshe","Martha White"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593515,"objectID":"098677398567120bbacee986b1f01543","permalink":"https://vincentliu3.github.io/publication/wangno-2022-no/","publishdate":"2023-06-12T18:11:55.243504Z","relpermalink":"/publication/wangno-2022-no/","section":"publication","summary":"","tags":[],"title":"No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL","type":"publication"},{"authors":["Somjit Nath","Vincent Liu","Alan Chan","Adam White","Martha White"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593516,"objectID":"87f752983a07b533d6f8b0da3a992db4","permalink":"https://vincentliu3.github.io/publication/nath-2020-training/","publishdate":"2023-06-12T18:11:55.983895Z","relpermalink":"/publication/nath-2020-training/","section":"publication","summary":"","tags":[],"title":"Training Recurrent Neural Networks Online by Learning Explicit State Variables","type":"publication"},{"authors":["Vincent Liu","Raksha Kumaraswamy","Lei Le","Martha White"],"categories":[],"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593516,"objectID":"dec988fa877255bab09145faade63a2e","permalink":"https://vincentliu3.github.io/publication/liu-2019-utility/","publishdate":"2023-06-12T18:11:56.327216Z","relpermalink":"/publication/liu-2019-utility/","section":"publication","summary":"","tags":[],"title":"The utility of sparse representations for control in reinforcement learning","type":"publication"},{"authors":["Andrew Jacobsen","Vincent Liu","Roshan Shariff","Adam White","Martha White"],"categories":[],"content":"","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593517,"objectID":"f67e8206906bf9f5afaf71b854b37b65","permalink":"https://vincentliu3.github.io/publication/jacobsen-2019-value/","publishdate":"2023-06-02T18:11:56.720178Z","relpermalink":"/publication/jacobsen-2019-value/","section":"publication","summary":"","tags":[],"title":"A Value Function Basis for Nexting and Multi-step Prediction","type":"publication"},{"authors":["Brendan Bennett","Wesley Chung","Muhammad Zaheer","Vincent Liu"],"categories":[],"content":"","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593517,"objectID":"754bebe5acaafe1f2890093b2348b2a4","permalink":"https://vincentliu3.github.io/publication/bennett-2019-incrementally/","publishdate":"2023-06-12T18:11:57.114005Z","relpermalink":"/publication/bennett-2019-incrementally/","section":"publication","summary":"","tags":[],"title":"Incrementally Learning Functions of the Return","type":"publication"},{"authors":["Wen-Hao Chen","Chin-Chi Hsu","Yi-An Lai","Vincent Liu","Mi-Yen Yeh","Shou-De Lin"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686593517,"objectID":"da2847b0560db00da0b6bfcb59ab37cc","permalink":"https://vincentliu3.github.io/publication/chen-2020-attribute/","publishdate":"2023-06-12T18:11:57.47474Z","relpermalink":"/publication/chen-2020-attribute/","section":"publication","summary":"","tags":[],"title":"Attribute-aware recommender system based on collaborative filtering: Survey and classification","type":"publication"}]